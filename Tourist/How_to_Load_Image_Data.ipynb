{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data_Loader\n",
    "## 我们将使用 TensorFlow 2 构建数据导入Pipeline\n",
    "#### 1、分类任务\n",
    "#### 2、分割任务\n",
    "#### 3、随机匹配"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 分类任务\n",
    "1、 文件夹的目录形式如下\n",
    "\n",
    "Father_Dir\n",
    "- Apple\n",
    "    - 1.jpg\n",
    "    - 2.jpg\n",
    "- Origin\n",
    "    - 1.jpg\n",
    "    - 2.jpg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: C:\\\\Users\\\\liuye\\\\Desktop\\\\Father_Dir/*/*.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m Father_Dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mliuye\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDesktop\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mFather_Dir/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# shuffle 默认为Ture\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m files_path_Dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43mFather_Dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m*/*.jpg\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(files_path_Dataset)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(正确)将shuffle设置为False，遍历迭代器：\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1336\u001B[0m, in \u001B[0;36mDatasetV2.list_files\u001B[1;34m(file_pattern, shuffle, seed, name)\u001B[0m\n\u001B[0;32m   1329\u001B[0m condition \u001B[38;5;241m=\u001B[39m math_ops\u001B[38;5;241m.\u001B[39mgreater(array_ops\u001B[38;5;241m.\u001B[39mshape(matching_files)[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   1330\u001B[0m                              name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmatch_not_empty\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1332\u001B[0m message \u001B[38;5;241m=\u001B[39m math_ops\u001B[38;5;241m.\u001B[39madd(\n\u001B[0;32m   1333\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo files matched pattern: \u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1334\u001B[0m     string_ops\u001B[38;5;241m.\u001B[39mreduce_join(file_pattern, separator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m), name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1336\u001B[0m assert_not_empty \u001B[38;5;241m=\u001B[39m \u001B[43mcontrol_flow_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAssert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcondition\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msummarize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43massert_not_empty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1338\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mcontrol_dependencies([assert_not_empty]):\n\u001B[0;32m   1339\u001B[0m   matching_files \u001B[38;5;241m=\u001B[39m array_ops\u001B[38;5;241m.\u001B[39midentity(matching_files)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:160\u001B[0m, in \u001B[0;36mAssert\u001B[1;34m(condition, data, summarize, name)\u001B[0m\n\u001B[0;32m    158\u001B[0m     xs \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mconvert_n_to_tensor(data)\n\u001B[0;32m    159\u001B[0m     data_str \u001B[38;5;241m=\u001B[39m [_summarize_eager(x, summarize) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m xs]\n\u001B[1;32m--> 160\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mInvalidArgumentError(\n\u001B[0;32m    161\u001B[0m         node_def\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    162\u001B[0m         op\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    163\u001B[0m         message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to be true. Summarized data: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m    164\u001B[0m         (condition, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(data_str)))\n\u001B[0;32m    165\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mname_scope(name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAssert\u001B[39m\u001B[38;5;124m\"\u001B[39m, [condition, data]) \u001B[38;5;28;01mas\u001B[39;00m name:\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: C:\\\\Users\\\\liuye\\\\Desktop\\\\Father_Dir/*/*.jpg'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "Father_Dir = r'C:\\Users\\liuye\\Desktop\\Father_Dir/'\n",
    "# shuffle 默认为Ture\n",
    "files_path_Dataset = tf.data.Dataset.list_files(Father_Dir + r'*/*.jpg', shuffle=False)\n",
    "print(files_path_Dataset)\n",
    "print('(正确)将shuffle设置为False，遍历迭代器：')\n",
    "for i in files_path_Dataset.as_numpy_iterator():\n",
    "    print(i)\n",
    "print('(错误)将shuffle设置为False，遍历迭代器：')\n",
    "for i in range(4):\n",
    "    print(files_path_Dataset.as_numpy_iterator().next())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在我们能够得到不断输出jpg图像的路径的迭代器files_path\n",
    "\n",
    "接下来我们可以编写load_function函数从文件名导入图像文件,transform函数预处理图像"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 先构建image与label的配对\n",
    "image_label_Dict = {\"Apple\": 0, \"Orange\": 1}\n",
    "\n",
    "def transform(image):\n",
    "    image = image / 255\n",
    "    return image\n",
    "\n",
    "def load_function(img_path, target_size=(32, 32), Transform=None, Dict=None):\n",
    "    # 从路径导入图像，这一步可以使用opncv, tensorflow, PIL等库，可以根据自己要进行的图像处理库进行选择\n",
    "    if Transform is None:\n",
    "        Transform = transform\n",
    "    if Dict is None:\n",
    "        Dict = image_label_Dict\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    if target_size != (0, 0):\n",
    "        image = tf.image.resize(image, target_size)\n",
    "    if Transform:\n",
    "        image = Transform(image)\n",
    "    # 如果是有监督，对应的image还需要Label\n",
    "    # 在此案例中，Image分为了Apple与Orange,所以我们可以构建一个字典来进行标注\n",
    "    label = tf.constant(2, dtype=tf.int8)\n",
    "    if Dict:\n",
    "        for key in Dict.keys():\n",
    "            if tf.strings.regex_full_match(img_path, \".*{}.*\".format(key)):\n",
    "                label = tf.constant(Dict.get(key), dtype=tf.int8)\n",
    "\n",
    "    return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train = files_path_Dataset.map(load_function)\n",
    "for i, (images, labels) in enumerate(data_train.take(4)):\n",
    "    print(images.shape)\n",
    "    print(labels.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 分割任务\n",
    "1、 文件夹的目录形式如下\n",
    "\n",
    "Father_Dir\n",
    "\n",
    "- Image\n",
    "    - 1.jpg\n",
    "    - 2.jpg\n",
    "- Segmentation\n",
    "    - 1.png\n",
    "    - 2.png\n",
    "\n",
    "此时1.jpg对应1.png，2.jpg对应2.png\n",
    "此时就不能使用`tf.data.Dataset.list_files`，因为一方面Image与Label是分离的，另一方面，Image与Label不是无序的，是存在相互对应的关系，\n",
    "所以此时应该采取另外一种方式，即借助txt文件，txt的一行有两个参数分别对应Image与Label，中间用 `,` 隔开，到时候直接对txt文件进行处理就能构建好\n",
    "pipeline。\n",
    "\n",
    "**但是在示例中，Segmentation的文件格式jpg**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_txt = r'L:\\ALASegmentationNets_v2\\Data\\Stage_4\\train.txt'\n",
    "train_img_file = r'L:\\ALASegmentationNets_v2\\Data\\Stage_4\\train\\img/'\n",
    "train_label_file = r'L:\\ALASegmentationNets_v2\\Data\\Stage_4\\train\\mask/'\n",
    "files_txt = np.loadtxt(file_txt, delimiter=',', dtype=bytes, encoding='utf-8')\n",
    "path_ds = tf.data.Dataset.from_tensor_slices(files_txt)\n",
    "\n",
    "print(path_ds.as_numpy_iterator().next())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 此时需要对load_function进行重写才能满足条件\n",
    "# 对Transform要认真构造\n",
    "\n",
    "transform_image = transform\n",
    "transform_label = transform\n",
    "\n",
    "def load_function(img_label_path, target_size=(448, 448),\n",
    "                  Transform_image = transform_image,\n",
    "                  Transform_Label = transform_label\n",
    "                  ):\n",
    "    # 从路径导入图像，这一步可以使用opncv, tensorflow, PIL等库，可以根据自己要进行的图像处理库进行选择\n",
    "    # 注意图像文件格式！\n",
    "    img_path, label_path = img_label_path[0], img_label_path[1]\n",
    "\n",
    "    image = tf.io.read_file(train_img_file + img_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "\n",
    "    label = tf.io.read_file(train_label_file + label_path)\n",
    "    label = tf.image.decode_jpeg(label)\n",
    "\n",
    "    if target_size != (0, 0):\n",
    "        image = tf.image.resize(image, target_size)\n",
    "    if Transform_image:\n",
    "        image = Transform_image(image)\n",
    "    if Transform_Label:\n",
    "        label = Transform_Label(label)\n",
    "\n",
    "    return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train = path_ds.map(load_function)\n",
    "for i, (images, labels) in enumerate(data_train.take(4)):\n",
    "    print(images.shape)\n",
    "    print(labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cycle_GAN 非对称数据集\n",
    "3、 文件夹的目录形式如下\n",
    "\n",
    "Father_Dir\n",
    "- Apple\n",
    "    - 1.jpg\n",
    "    - 2.jpg\n",
    "- Origin\n",
    "    - 1.jpg\n",
    "    - 2.jpg\n",
    "\n",
    "假如我们想训练一个神经网络能够把苹果变成橘子，也能把橘子变成苹果，此时虽然说Apple与Orange是对应的，但是在各子类中图像是可以无序的，\n",
    "此时应该如何构建Pipeline呢？\n",
    "\n",
    "其实非常简单，把Apple看成Image， Origin看出Label，将内部状态设定为无序就可以"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Father_Dir = r'C:\\Users\\liuye\\Desktop\\Father_Dir/'\n",
    "# shuffle 默认为Ture\n",
    "# 所以每次运行，其对应的文件都不一样，但类是相对应的\n",
    "Apple_files_path_Dataset = tf.data.Dataset.list_files(Father_Dir + r'Apple/*.jpg', shuffle=True)\n",
    "Orange_files_path_Dataset = tf.data.Dataset.list_files(Father_Dir + r'Orange/*.jpg', shuffle=True)\n",
    "for i in range(3):\n",
    "    for j in tf.data.Dataset.zip((Apple_files_path_Dataset, Orange_files_path_Dataset)):\n",
    "        print(j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "此时我们设置好相应transform函数就能完成图像的载入了\n",
    "请注意与第二部分的load_function的差别"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_function(img_path, target_size=(448, 448),\n",
    "                  Transform_image = transform_image,\n",
    "                  ):\n",
    "    # 从路径导入图像，这一步可以使用opncv, tensorflow, PIL等库，可以根据自己要进行的图像处理库进行选择\n",
    "    # 注意图像文件格式！\n",
    "\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "\n",
    "    if target_size != (0, 0):\n",
    "        image = tf.image.resize(image, target_size)\n",
    "    if Transform_image:\n",
    "        image = Transform_image(image)\n",
    "\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 注意这使用transform函数直接就是第二类的load_function\n",
    "# 我们来看看效果把\n",
    "\n",
    "data_train_Apple = Apple_files_path_Dataset.map(load_function)\n",
    "data_train_Orange = Orange_files_path_Dataset.map(load_function)\n",
    "data_train = tf.data.Dataset.zip((data_train_Apple, data_train_Orange))\n",
    "for i, (apple, orange) in enumerate(data_train.take(4)):\n",
    "    print(apple.shape)\n",
    "    print(orange.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tensorflow",
   "language": "python",
   "display_name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}